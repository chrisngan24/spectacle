{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem Definition\n",
    "\n",
    "https://en.wikipedia.org/wiki/German_tank_problem\n",
    "\n",
    "Lets set up the problem. In WW2, the allied forces are trying to estimate the number of german tanks. One idea is to use the power of statistics to complete this. Lets assume that during manufacturing, each tank got a serial number assigned to it. The serial numbers increase by sequence. To simplify this, the first tank is `1`, then `2`, and so forth. For `n` tanks, the serial number of the nth tank is `n`.\n",
    "\n",
    "So as the allied forces, we are able to capture a few of these tanks. Can we use the observed serial numbers to then estimate the number of tanks actually manufactured?\n",
    "\n",
    "We have to make a few simple assumptions:\n",
    "\n",
    "- First, as already stated, we assume that the serial numbers assigned to each tank is given in a numerically increasing order. (see above).\n",
    "- Second, we assume tanks are distributed in a uniform fashion. So the capture tanks are uniformly sampled by the manufacturing process. So the serial number observed is indpenent of each other.\n",
    "\n",
    "Before we go through the math, lets generate a sequence of `n` tanks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "N = 122\n",
    "def manufacture_n_tanks(n):\n",
    "    return np.arange(0,n, 1)\n",
    "tank_population = manufacture_n_tanks(N)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets \"capture\" a few of them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  4  27   6  73  14  57 120  91  11  58  32  84  18 109  12]\n"
     ]
    }
   ],
   "source": [
    "import scipy\n",
    "k = 15\n",
    "def capture_k_tanks(tank_population, k):\n",
    "    return np.random.choice(tank_population, replace=False, size=k)\n",
    "tank_sample = capture_k_tanks(tank_population, k)\n",
    "print tank_sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some notation to consider:\n",
    "- $N$ is the number of tanks in the total population. This is also the maximum value that can be observed\n",
    "- $k$ is the sample size (ie. tank_sample)\n",
    "- $m$ is the maximum serial number in the tank sample. \n",
    "\n",
    "There are some basic constraints:\n",
    "- $k \\leq N$ - since the sample size can never exceed the population\n",
    "- $k \\leq m \\leq N$ - since the largest number in the sample can never be less than the number of samples and can never be greater than the population (since N is the max number)\n",
    "\n",
    "To estimate the size of the population, lets consider the Frequentist method. Later on we can try the Bayesian method (TODO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Frequentist Method\n",
    "\n",
    "The intuition of the point estimate is to take the maximum observation in the sample scaled by the number of samples. This is derived by the Maximum Likelihood Estimation.\n",
    "\n",
    "First, consider a number $m$ s.t. $k \\leq m \\leq N$ and the probability that the number is the largest \n",
    "\n",
    "\n",
    "Let $X_i ~ U(!,N)$ and $X= (X_1,...,X_k)$. Each sample of $X_i$ is iid. Assume we are sampling from a discrete uniform distribution.\n",
    "\n",
    "\n",
    "$$ P(X=x|N) = \\frac{1}{N-1} $$\n",
    "\n",
    "If we consider then the MLE, ie the joint distribution of iid samples $X$. \n",
    "\n",
    "$$ P(X) = P(X=X_1) \\bullet P(X=X_2) ... \\bullet P(X=X_N)  = \\prod_{i} P(X=X_i|N) $$\n",
    "The likelihood function is\n",
    "$$ L(X|N) = \\arg\\max_{N} \\prod_{i=1}^{k} \\frac{1}{N-1} = (N-1)^{-k}$$ \n",
    "\n",
    "So what value of $N$ maximizes that function?\n",
    "\n",
    "$$ \\frac{ d \\ L(X|N)}{ d \\ N} = -\\frac{k}{N-1}^{k-1} < 0$$\n",
    "\n",
    "Since the derivative is negative, the maximum occurs when $N$ is as large as possible.\n",
    "From our observed data:\n",
    "\n",
    "$$ \\hat{N} = max(X)=m$$\n",
    "\n",
    "If we say $X_{(1)} < X_{(2)} < .. X_{(k)}$, then the $max(X)=X_{(k)}$ and $\\hat{N}=X_{(k)}$\n",
    "\n",
    "\n",
    "If we look at the bias of our estimate\n",
    "\n",
    "$$E(\\hat{N}) = E(X_{(k)}) = \\sum_{i=0}^{N} X_{(k=i)} P(X_{(k=i)})$$\n",
    "\n",
    "So what's the probability of $m$?\n",
    "Given the number of samples $k$ and the population size of $N$, the probability that the random variable $m$ is the maximum of the sample is determined by the possible combinations of the samples. So for up to the value $m$, there are $\\begin{pmatrix} m \\\\ k \\end{pmatrix}  $ possible samples to be observed. However, we only care about the few where $m$ is part of that sample, so the difference of $\\begin{pmatrix} m-1 \\\\ k \\end{pmatrix}$. In total, there are $\\begin{pmatrix} N \\\\ k \\end{pmatrix}  $\n",
    "$$P(M=m|k,N) =\n",
    "\\begin{cases} \n",
    "      0 & m < k \\\\\n",
    "      \\frac\n",
    "          {\\begin{pmatrix} m \\\\ k \\end{pmatrix}-\\begin{pmatrix} m-1 \\\\ k \\end{pmatrix}}\n",
    "          {\\begin{pmatrix} N \\\\ k \\end{pmatrix}  } & k \\leq m \\leq N \\\\\n",
    "      0 & m > N\n",
    "\\end{cases}$$\n",
    "\n",
    "Lets simplify the piecewise function a bit.\n",
    "\n",
    "$$  \\frac\n",
    "  {\\begin{pmatrix} m \\\\ k \\end{pmatrix}-\\begin{pmatrix} m-1 \\\\ k \\end{pmatrix}}\n",
    "  {\\begin{pmatrix} N \\\\ k \\end{pmatrix}  } = \n",
    "  \\frac{\\frac{m!}{k!(m-k)!} - \\frac{(m-1)!}{k!(m-1-k)!}}\n",
    "      {\\frac{N!}{k!(N-k)!}} = \n",
    "  \\frac{\\frac{m!-(m-k)(m-1)!}{(m-k)!}}\n",
    "      {\\frac{N!}{(N-k)!}} =\n",
    "  \\frac{\\frac{m!-m(m-1)! + k(m-1)!}{(m-k)!}}\n",
    "      {\\frac{N!}{(N-k)!}}   =\n",
    "  \\frac{\\frac{k(m-1)!}{(m-k)!}}\n",
    "      {\\frac{N!}{(N-k)!}}   \n",
    "      $$\n",
    "      \n",
    "      \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    \n",
    "$$ E(\\hat{N}) = E(m) = \\sum_{m=k}^{N} m P(m) = \\sum_{m=k}^{N} m \\frac{\\frac{k(m-1)!}{(m-k)!}}\n",
    "      {\\frac{N!}{(N-k)!}}     = \\frac{k(N+1)}{k+1} \\neq N $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then see that the estimate for $N$ is biased! Lets rewrite the expected value of the estimate to get the unbiased form of \n",
    "\n",
    "$$ E(m) = \\frac{k(N+1)}{k+1} $$\n",
    "\n",
    "We can then rewrite the estimate of $N$ as:\n",
    "\n",
    "$$ E(m)(1 + k^{-1})-1 = E[m(1 + k^{-1})-1] = E[\\hat{N}] $$\n",
    "\n",
    "$$ \\hat{N} = m(1+k^{-1})-1 $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimate of N from sample data: 127.0\n"
     ]
    }
   ],
   "source": [
    "def estimate_n(samples):\n",
    "    return max(samples)*(1+1/float(len(samples))) - 1\n",
    "print \"Estimate of N from sample data:\" , estimate_n(tank_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also compute the confidence interval of our estimates.. but we'll save that for a later day :)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
