{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a collection of notes from the reading of \"Elements of Statistical Learning\" - chapter 2 \"Overview of Supervised Learning\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Statistical Decision Theory\n",
    "\n",
    "A lot of Model building/inference/etc. is choosing a model that best minimizes some error. If our feature variables are $X \\epsilon \\mathbb{R}^p$ and our response variate is $Y \\epsilon \\mathbb{R}$ we assume that the variables are produced by a joint distribution $(X,Y) \\sim P(X,Y)=P(Y|X)P(X)$. We say that $Y$ is a conditional probability produced by some function of $X$ with some noise. In othe words:\n",
    "\n",
    "$$Y \\sim P(Y|X) = f(x) + N(\\sigma) $$\n",
    "\n",
    "We can then say that the general learning problem is finding a criterion that minimizes a _loss function_ $L(Y, f(X))$, or the **Expected Predicted Error** (EPE).\n",
    "\n",
    "$$EPE(f) = E(L(Y - f(X))) = \\int L([y - f(x)])P(dx, dy) $$\n",
    "\n",
    "This can be conditioned on $X$ and since it is a joint distribution we can rewrite it as\n",
    "\n",
    "$$ EPE(f) = E_X E_{Y|X}(L(Y - f(x))|X) $$\n",
    "\n",
    "To minimize the EPE then, the problem becomes minimizing $E_{Y|X}$ by solving:\n",
    "$$ f(x) = E(Y|X=x)$$\n",
    "\n",
    "In other words, $f(x) \\approx y$. \n",
    "\n",
    "Some important insights:\n",
    "- Solving for f(x) will never give you zero error as there is inherently noise in the distribution of P(Y|X). \n",
    "- Values of $Y$ in the population are conditioned on the value of $X$. Therefore, the function that approximates $Y$ is also conditioned on the value of $X$. In real world situations, we are observing $X$ to predict $Y$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regression Models\n",
    "\n",
    "Regression looks at solving a function that outputs a continuous variable. There are variations in what is an appropriate loss function. Some ideas:\n",
    "\n",
    "- Squared Error ($L_2$) : $(Y - f(X))^2$\n",
    "- Absolute Error ($L_1$): $ |Y - f(X)|$\n",
    "\n",
    "### Classificaiton Models\n",
    "\n",
    "Classification looks at solving a function that outputs a discrete variable from a defined set. For notation, classes are noted as $G$ instead of $Y$. There are variations in appropriate loss function. Some ideas:\n",
    "\n",
    "- Zero-One Loss : $I(f(x) \\neq G)$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Statistical Models\n",
    "\n",
    "\n",
    "#### Regression (Function Estimation)\n",
    "\n",
    "The general problem is do find an approximateion of $\\hat{f(x)}$ to the function of $f(x)$ which underlies the predictive relation between features and response. For most systems, $(X,Y)$ is not a deterministic system. To then approximate $Y$, we use an additive model to represent the stocahastic noise.\n",
    "\n",
    "$$ Y = f(X) + \\varepsilon$$\n",
    "\n",
    "Where $\\varepsilon$ is noise independent of $X$. The noise term captures errors resulting from measurement, variations in components and infinite possible initial states. \n",
    "\n",
    "#### Classification (Maximizing Likelihood)\n",
    "\n",
    "For classification problems, there is no random error associated with the output $G$. Furthermore, the target function being estimated is less so of a function itself, but the probability of class $G=g$ conditioned on the variable $X$. Classification is very much deciding class $g$ that maximizes the probability.\n",
    "\n",
    "$$ G = \\max_{g} P(G=g|X)$$\n",
    "\n",
    "Hence, the variance of $G$ is also dependent on the probility of g(x): $Var(Y|X=x) = p(x)[1-p(x)]$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
